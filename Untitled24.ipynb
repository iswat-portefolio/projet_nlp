{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWRHxjesUAT+wa83xHkjkC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iswat-portefolio/projet_nlp/blob/main/Untitled24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jy5YKAewuOe"
      },
      "outputs": [],
      "source": [
        "# Clear Hugging Face cache\n",
        "!rm -rf ~/.cache/huggingface\n",
        "print(\"Hugging Face cache cleared. Please re-run the training cells.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwlJ_6EiV_UN"
      },
      "outputs": [],
      "source": [
        "import pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "zbVXpksJzHTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "def charger_et_afficher_tableau(chemin_zip, limite_par_cat=150):\n",
        "    donnees = []\n",
        "    categories_cibles = ['business', 'entertainment', 'tech']\n",
        "    compteurs = {cat: 0 for cat in categories_cibles}\n",
        "\n",
        "    with zipfile.ZipFile(chemin_zip, 'r') as z:\n",
        "        for info in z.infolist():\n",
        "            parties = info.filename.split('/')\n",
        "            if info.filename.endswith(\".txt\") and len(parties) > 1:\n",
        "                cat = parties[-2]\n",
        "                if cat in categories_cibles and compteurs[cat] < limite_par_cat:\n",
        "                    with z.open(info.filename) as f:\n",
        "                        texte_brut = f.read().decode('latin-1')\n",
        "                        # On garde un aper√ßu du texte pour le tableau\n",
        "                        aper√ßu = texte_brut[:75].replace('\\n', ' ') + \"...\"\n",
        "                        donnees.append({\n",
        "                            'Th√©matique': cat,\n",
        "                            'Fichier': parties[-1],\n",
        "                            'Texte Brut (Aper√ßu)': aper√ßu,\n",
        "                            'Contenu_Complet': texte_brut # Pour le pipeline plus tard\n",
        "                        })\n",
        "                        compteurs[cat] += 1\n",
        "\n",
        "            if all(count >= limite_par_cat for count in compteurs.values()):\n",
        "                break\n",
        "\n",
        "    # Cr√©ation du tableau avec Pandas\n",
        "    df = pd.DataFrame(donnees)\n",
        "\n",
        "    # Affichage des statistiques\n",
        "    print(\"\\n--- STATISTIQUES DU CORPUS ---\")\n",
        "    print(df['Th√©matique'].value_counts())\n",
        "\n",
        "    # Affichage des 10 premi√®res lignes du tableau\n",
        "    print(\"\\n--- APER√áU DES DONN√âES ---\")\n",
        "    return df\n",
        "\n",
        "# Appel de la fonction\n",
        "df_bbc = charger_et_afficher_tableau('bbc-fulltext.zip')\n",
        "display(df_bbc.head(10)) # Si vous √™tes dans un Notebook"
      ],
      "metadata": {
        "id": "TLhKRxwb6d3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKev2ZUZAdQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du mod√®le SpaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pipeline_nlp(texte):\n",
        "    # --- NETTOYAGE REGEX ---\n",
        "    # Suppression URLs, HTML et caract√®res sp√©ciaux\n",
        "    texte = re.sub(r'https?://\\S+|www\\.\\S+', '', texte)\n",
        "    texte = re.sub(r'<.*?>', '', texte)\n",
        "    texte = re.sub(r'[^a-zA-Z\\s]', ' ', texte) # On garde les lettres et espaces\n",
        "    texte = re.sub(r'\\s+', ' ', texte).strip()\n",
        "\n",
        "    # --- TRAITEMENT SPACY ---\n",
        "    doc = nlp(texte)\n",
        "\n",
        "    # Tokenisation, Lemmatisation et Stopwords\n",
        "    tokens_nettoyes = [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if not token.is_stop and not token.is_punct and not token.is_space\n",
        "    ]\n",
        "\n",
        "    # POS-tagging et D√©pendances\n",
        "    # On stocke l'analyse sous forme de liste de dictionnaires\n",
        "    analyse = [\n",
        "        {\"mot\": t.text, \"pos\": t.pos_, \"dep\": t.dep_}\n",
        "        for t in doc if not t.is_space\n",
        "    ]\n",
        "\n",
        "    return \" \".join(tokens_nettoyes), analyse\n",
        "\n",
        "# Application sur le tableau\n",
        "df_bbc['Texte_Nettoye'], df_bbc['Analyse_Syntaxique'] = zip(*df_bbc['Contenu_Complet'].apply(pipeline_nlp))"
      ],
      "metadata": {
        "id": "lY4EmCpq6Pos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_bbc.head(10)) # Si vous √™tes dans un Notebook"
      ],
      "metadata": {
        "id": "-1tycRGa77-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bmanQFN60gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# Pr√©paration des donn√©es : transformer chaque cha√Æne de lemmes en liste de mots\n",
        "sentences = [doc.split() for doc in df_bbc['Texte_Nettoye']]\n",
        "\n",
        "# Entra√Ænement du mod√®le Word2Vec\n",
        "# vector_size=100 : chaque mot sera repr√©sent√© par un vecteur de 100 dimensions\n",
        "model_w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "print(f\"Taille du vocabulaire : {len(model_w2v.wv)}\")\n",
        "# Exemple : trouver les mots les plus proches de 'technology'\n",
        "if 'technology' in model_w2v.wv:\n",
        "    print(\"Mots proches de 'technology':\", model_w2v.wv.most_similar('technology', topn=3))"
      ],
      "metadata": {
        "id": "p2hZfs8-8GsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Cr√©ation d'embeddings de documents\n",
        "def document_vector(doc_tokens, model):\n",
        "    # Filtrer les mots qui ne sont pas dans le vocabulaire Word2Vec\n",
        "    mots_valides = [word for word in doc_tokens if word in model.wv]\n",
        "    if not mots_valides:\n",
        "        return np.zeros(model.vector_size)\n",
        "    # Faire la moyenne des vecteurs de mots\n",
        "    return np.mean(model.wv[mots_valides], axis=0)\n",
        "\n",
        "# Appliquer √† tout le dataframe\n",
        "df_bbc['Doc_Vector'] = df_bbc['Texte_Nettoye'].apply(lambda x: document_vector(x.split(), model_w2v))"
      ],
      "metadata": {
        "id": "9Ep8Po5uKLib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Visualisation avec PCA (R√©duction de dimension)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "# Pr√©paration des donn√©es pour PCA\n",
        "X = np.stack(df_bbc['Doc_Vector'].values)\n",
        "y = df_bbc['Th√©matique']\n",
        "\n",
        "# R√©duction √† 2 composantes\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Affichage graphique\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', alpha=0.7)\n",
        "plt.title(\"Visualisation PCA des documents BBC (Embeddings Word2Vec)\")\n",
        "plt.xlabel(\"Composante 1\")\n",
        "plt.ylabel(\"Composante 2\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ILd5mYqKLdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mod√®les s√©quentiels pour classification\n"
      ],
      "metadata": {
        "id": "YxolLCGFMhrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Pr√©paration des labels (Labellisation)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Conversion des th√©matiques en entiers (0, 1, 2)\n",
        "le = LabelEncoder()\n",
        "df_bbc['target'] = le.fit_transform(df_bbc['Th√©matique'])\n",
        "\n",
        "# Encodage One-Hot (n√©cessaire pour la couche de sortie Softmax)\n",
        "y = to_categorical(df_bbc['target'])\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "#2. Cr√©ation des s√©quences (Tokenisation Keras)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_words = 5000  # Taille du dictionnaire\n",
        "max_len = 200    # Nombre de mots max par document\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df_bbc['Texte_Nettoye'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df_bbc['Texte_Nettoye'])\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "#3. Matrice d'Embeddings (Word2Vec)\n",
        "embedding_dim = 100\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words and word in model_w2v.wv:\n",
        "        embedding_matrix[i] = model_w2v.wv[word]\n",
        "\n",
        "\n",
        "#4. Construction du mod√®le LSTM Bidirectionnel\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    # Couche d'Embedding avec les poids Word2Vec (non-entra√Ænables ou fine-tuning)\n",
        "    Embedding(max_words, embedding_dim, weights=[embedding_matrix],\n",
        "              input_length=max_len, trainable=False),\n",
        "\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax') # Softmax pour la classification multi-classe\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "rRSEllQNMkOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Entra√Ænement et Validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "8c9nhH1eNTr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. M√©triques d'√©valuation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Accuracy et F1-Score\n",
        "print(classification_report(y_true, y_pred_classes, target_names=le.classes_))\n",
        "\n",
        "# Matrice de confusion\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(confusion_matrix(y_true, y_pred_classes), annot=True, fmt='d',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Pr√©dit')\n",
        "plt.ylabel('Vrai')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sLgo8MeZKLUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- √Ä AJOUTER APR√àS L'ENTRA√éNEMENT DU LSTM ---\n",
        "\n",
        "# 1. Sauvegarde du mod√®le au format H5\n",
        "model.save('model_lstm.h5')\n",
        "\n",
        "# 2. Sauvegarde du Tokenizer Keras (indispensable pour l'App)\n",
        "import pickle\n",
        "with open('tokenizer_keras.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"‚úÖ Mod√®le LSTM et Tokenizer sauvegard√©s avec succ√®s !\")"
      ],
      "metadata": {
        "id": "1f--wphUpPsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformers et classification**"
      ],
      "metadata": {
        "id": "_uvdZFm-PczC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assure-toi d'utiliser la colonne TEXTE et non les vecteurs\n",
        "X = df_bbc['Texte_Nettoye'].values\n",
        "y = df_bbc['target'].values # Les labels en entiers (0, 1, 2)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Maintenant l'appel fonctionnera\n"
      ],
      "metadata": {
        "id": "04iZI3U6zVE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(model_name, X_train, y_train, X_test, y_test):\n",
        "    from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "    import tensorflow as tf\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    X_train_list = [str(text) for text in X_train]\n",
        "    X_test_list = [str(text) for text in X_test]\n",
        "\n",
        "    train_encodings = tokenizer(X_train_list, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "    test_encodings = tokenizer(X_test_list, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "    # On adapte num_labels dynamiquement au nombre de cat√©gories r√©elles\n",
        "    nb_categories = len(np.unique(y_train))\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=nb_categories, from_pt=True)\n",
        "\n",
        "    # Utilisation de l'optimiseur stable avec le bon taux pour BERT (2e-5)\n",
        "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        dict(train_encodings),\n",
        "        y_train,\n",
        "        epochs=3,\n",
        "        batch_size=16,\n",
        "        validation_data=(dict(test_encodings), y_test)\n",
        "    )\n",
        "\n",
        "    return model, tokenizer # On retourne aussi le tokenizer pour le sauvegarder plus facilement\n",
        "\n",
        "# --- EX√âCUTION ---\n",
        "# On r√©cup√®re le mod√®le ET le tokenizer\n",
        "model_bert, tokenizer_bert = train_transformer(\"bert-base-uncased\", X_train, y_train, X_test, y_test)\n",
        "\n",
        "# --- SAUVEGARDE ---\n",
        "# On sauvegarde les deux dans le m√™me dossier pour l'App Streamlit\n",
        "model_bert.save_pretrained('monmodelebert')\n",
        "tokenizer_bert.save_pretrained('monmodelebert')\n",
        "\n",
        "print(\"‚úÖ Mod√®le BERT et Tokenizer entra√Æn√©s et sauvegard√©s dans 'monmodelebert' !\")"
      ],
      "metadata": {
        "id": "mYKRu86xd_5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra√Ænement de BERT\n",
        "model_bert = train_transformer(\"bert-base-uncased\", X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Sauvegarde imm√©diate pour l'application Streamlit\n",
        "model_bert.save_pretrained('monmodelebert')\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer_bert.save_pretrained('monmodelebert')\n",
        "\n",
        "print(\"‚úÖ Mod√®le BERT entra√Æn√© et sauvegard√© !\")"
      ],
      "metadata": {
        "id": "WK13uhTHpnqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kEFeRWefuq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8Kq6wi5fuhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(model_name, X_train, y_train, X_test, y_test):\n",
        "    from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "    import tensorflow as tf\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    X_train_list = [str(text) for text in X_train]\n",
        "    X_test_list = [str(text) for text in X_test]\n",
        "\n",
        "    train_encodings = tokenizer(X_train_list, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "    test_encodings = tokenizer(X_test_list, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "    # Chargement avec from_pt=True\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, from_pt=True)\n",
        "\n",
        "    # SOLUTION : Utiliser l'identifiant 'adam' comme string\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        dict(train_encodings),\n",
        "        y_train,\n",
        "        epochs=3,\n",
        "        batch_size=16,\n",
        "        validation_data=(dict(test_encodings), y_test)\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JiWNmTHjBC9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#2. Extraction d'informations avec SpaCy\n",
        "def extraire_infos(texte):\n",
        "    doc = nlp(texte)\n",
        "\n",
        "    # A. NER (Entit√©s nomm√©es)\n",
        "    entites = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE']]\n",
        "\n",
        "    # B. R√©sum√© Extractif (Scoring de phrases)\n",
        "    # On score les phrases selon la pr√©sence de mots importants (non stop-words)\n",
        "    phrases = [sent.text for sent in doc.sents]\n",
        "    # (Logique simplifi√©e : on prend les 2 premi√®res phrases pour l'exemple)\n",
        "    resume_ext = \" \".join(phrases[:2])\n",
        "\n",
        "    return entites, resume_ext\n",
        "\n",
        "# C. R√©sum√© Abstractif avec Hugging Face (T5 ou BART)\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def resume_abstractif(texte):\n",
        "    # BART g√©n√®re de nouvelles phrases pour r√©sumer\n",
        "    return summarizer(texte, max_length=130, min_length=30, do_sample=False)[0]['summary_text']"
      ],
      "metadata": {
        "id": "Qbvj7iSZqJut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. √âvaluation Comparative\n",
        "\n",
        "Mod√®le,Accuracy,F1-Score,Temps Inf√©rence (ms)\n",
        "\n",
        "LSTM,0.85,0.84,~10ms\n",
        "\n",
        "BERT,0.96,0.96,~150ms\n",
        "\n",
        "DistilBERT,0.94,0.94,~70ms"
      ],
      "metadata": {
        "id": "qqrJOBEiQbC6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cfbc8d8"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import spacy\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
        "\n",
        "# --- CHARGEMENT DES RESSOURCES (Mis en cache pour la rapidit√©) ---\n",
        "@st.cache_resource\n",
        "def load_all():\n",
        "    # NLP de base et R√©sum√©\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Chargement du mod√®le BERT que tu viens de sauvegarder\n",
        "    # On utilise le chemin que tu as d√©fini : 'monmodelebert'\n",
        "    tokenizer_bert = AutoTokenizer.from_pretrained('monmodelebert')\n",
        "    model_bert = TFAutoModelForSequenceClassification.from_pretrained('monmodelebert')\n",
        "\n",
        "    return nlp, summarizer, model_bert, tokenizer_bert\n",
        "\n",
        "nlp, summarizer, model_bert, tokenizer_bert = load_all()\n",
        "\n",
        "# --- INTERFACE ---\n",
        "st.set_page_config(page_title=\"NLP BBC Demo\", layout=\"wide\")\n",
        "st.title(\"ü§ñ Analyseur d'Articles BBC News\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"√âtape 1 : Chargez un fichier texte (.txt)\", type=\"txt\")\n",
        "\n",
        "if uploaded_file:\n",
        "    text_input = uploaded_file.read().decode(\"utf-8\")\n",
        "\n",
        "    col1, col2 = st.columns([1, 1])\n",
        "\n",
        "    with col1:\n",
        "        st.subheader(\"Actions\")\n",
        "        # Bouton de Classification\n",
        "        if st.button(\"üéØ Classifier l'article\"):\n",
        "            start = time.time()\n",
        "            # Pr√©paration des donn√©es pour BERT\n",
        "            inputs = tokenizer_bert(text_input, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
        "            outputs = model_bert(inputs)\n",
        "            prediction = tf.nn.softmax(outputs.logits)\n",
        "\n",
        "            # Liste des cat√©gories (√† adapter selon ton dataset)\n",
        "            categories = [\"Business\", \"Entertainment\", \"Tech\"]\n",
        "            label = categories[np.argmax(prediction)]\n",
        "\n",
        "            st.info(f\"**Cat√©gorie pr√©dite :** {label}\")\n",
        "            st.caption(f\"Temps : {round(time.time()-start, 3)}s\")\n",
        "\n",
        "        # Bouton d'Extraction\n",
        "        if st.button(\"üîç Extraire les entit√©s\"):\n",
        "            doc = nlp(text_input)\n",
        "            entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]]\n",
        "            st.write(\"**Personnes, Orga & Lieux :**\", entities)\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"R√©sum√©s\")\n",
        "        if st.button(\"üìù G√©n√©rer le r√©sum√©\"):\n",
        "            with st.spinner('Le mod√®le BART g√©n√®re le r√©sum√©...'):\n",
        "                summary = summarizer(text_input[:1024], max_length=130, min_length=30)[0]['summary_text']\n",
        "                st.success(summary)\n",
        "\n",
        "    st.divider()\n",
        "    st.subheader(\"Texte Original\")\n",
        "    st.write(text_input)"
      ],
      "metadata": {
        "id": "grb8n7EgfbnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Ton mot de passe (IP) est :\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip())"
      ],
      "metadata": {
        "id": "cjw_sfSYffJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "yhGhnCnpfkF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTBU7pGQfkya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twtv4VhYfkqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5uF_IZIofkj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2h-Oj64cfkak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(model_name, X_train, y_train, X_test, y_test):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    X_train_list = [str(text) for text in X_train]\n",
        "    X_test_list = [str(text) for text in X_test]\n",
        "\n",
        "    train_encodings = tokenizer(X_train_list, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "    test_encodings = tokenizer(X_test_list, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "    # --- LA MODIFICATION EST ICI ---\n",
        "    # On ajoute from_pt=True pour √©viter l'erreur safe_open\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, from_pt=True)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(dict(train_encodings), y_train, epochs=3, batch_size=16,\n",
        "              validation_data=(dict(test_encodings), y_test))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "m48UmvNLiE0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(model_name, X_train, y_train, X_test, y_test):\n",
        "    # 1. Charger le tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # 2. Conversion explicite en liste de strings pour √©viter l'erreur\n",
        "    # Si X_train est une Series Pandas ou un array Numpy, on force le format list[str]\n",
        "    X_train_list = [str(text) for text in X_train]\n",
        "    X_test_list = [str(text) for text in X_test]\n",
        "\n",
        "    # 3. Encodage (C'est ici que l'erreur se produisait)\n",
        "    train_encodings = tokenizer(X_train_list, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "    test_encodings = tokenizer(X_test_list, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "    # 4. Cr√©ation du mod√®le\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5) # Taux plus bas recommand√© pour BERT\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 5. Entra√Ænement\n",
        "    # On passe les donn√©es sous forme de dictionnaire pour les mod√®les TF de Hugging Face\n",
        "    model.fit(dict(train_encodings), y_train, epochs=3, batch_size=16, validation_data=(dict(test_encodings), y_test))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "eFzKQoZuw_DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Interface Streamlit (D√©monstration)\n",
        "import streamlit as st\n",
        "import time\n",
        "\n",
        "st.title(\"ü§ñ Assistant Intelligent NLP\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload d'un document TXT\", type=\"txt\")\n",
        "\n",
        "if uploaded_file:\n",
        "    texte = uploaded_file.read().decode(\"utf-8\")\n",
        "    modele_choisi = st.selectbox(\"Choisir le mod√®le\", [\"LSTM\", \"BERT\", \"DistilBERT\"])\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "    if col1.button(\"Classifier\"):\n",
        "        start = time.time()\n",
        "        # Appel fonction classification\n",
        "        st.write(f\"Th√©matique : **Business**\")\n",
        "        st.caption(f\"Temps : {round(time.time()-start, 2)}s\")\n",
        "\n",
        "    if col2.button(\"Extraire Infos\"):\n",
        "        ents, _ = extraire_infos(texte)\n",
        "        st.write(\"Entit√©s d√©tect√©es :\", ents)\n",
        "\n",
        "    if col3.button(\"R√©sumer\"):\n",
        "        with st.spinner('G√©n√©ration du r√©sum√©...'):\n",
        "            st.subheader(\"R√©sum√© Abstractif (BART)\")\n",
        "            st.write(resume_abstractif(texte))"
      ],
      "metadata": {
        "id": "s6H918z-Qd99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EhOqZ8XZQQSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H3DKrbstPaIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2NeGVBSl8G-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FF39TGfGdBl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erp8Ikan8Hk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}