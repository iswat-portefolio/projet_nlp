{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iswat-portefolio/projet_nlp/blob/main/Copie_de_Untitled24_(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jy5YKAewuOe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwlJ_6EiV_UN"
      },
      "outputs": [],
      "source": [
        "import pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbVXpksJzHTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "def charger_et_afficher_tableau(chemin_zip, limite_par_cat=150):\n",
        "    donnees = []\n",
        "    categories_cibles = ['business', 'entertainment', 'tech']\n",
        "    compteurs = {cat: 0 for cat in categories_cibles}\n",
        "\n",
        "    with zipfile.ZipFile(chemin_zip, 'r') as z:\n",
        "        for info in z.infolist():\n",
        "            parties = info.filename.split('/')\n",
        "            if info.filename.endswith(\".txt\") and len(parties) > 1:\n",
        "                cat = parties[-2]\n",
        "                if cat in categories_cibles and compteurs[cat] < limite_par_cat:\n",
        "                    with z.open(info.filename) as f:\n",
        "                        texte_brut = f.read().decode('latin-1')\n",
        "                        # On garde un aper√ßu du texte pour le tableau\n",
        "                        aper√ßu = texte_brut[:75].replace('\\n', ' ') + \"...\"\n",
        "                        donnees.append({\n",
        "                            'Th√©matique': cat,\n",
        "                            'Fichier': parties[-1],\n",
        "                            'Texte Brut (Aper√ßu)': aper√ßu,\n",
        "                            'Contenu_Complet': texte_brut # Pour le pipeline plus tard\n",
        "                        })\n",
        "                        compteurs[cat] += 1\n",
        "\n",
        "            if all(count >= limite_par_cat for count in compteurs.values()):\n",
        "                break\n",
        "\n",
        "    # Cr√©ation du tableau avec Pandas\n",
        "    df = pd.DataFrame(donnees)\n",
        "\n",
        "    # Affichage des statistiques\n",
        "    print(\"\\n--- STATISTIQUES DU CORPUS ---\")\n",
        "    print(df['Th√©matique'].value_counts())\n",
        "\n",
        "    # Affichage des 10 premi√®res lignes du tableau\n",
        "    print(\"\\n--- APER√áU DES DONN√âES ---\")\n",
        "    return df\n",
        "\n",
        "# Appel de la fonction\n",
        "df_bbc = charger_et_afficher_tableau('bbc-fulltext.zip')\n",
        "display(df_bbc.head(10)) # Si vous √™tes dans un Notebook"
      ],
      "metadata": {
        "id": "TLhKRxwb6d3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKev2ZUZAdQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du mod√®le SpaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pipeline_nlp(texte):\n",
        "    # --- NETTOYAGE REGEX ---\n",
        "    # Suppression URLs, HTML et caract√®res sp√©ciaux\n",
        "    texte = re.sub(r'https?://\\S+|www\\.\\S+', '', texte)\n",
        "    texte = re.sub(r'<.*?>', '', texte)\n",
        "    texte = re.sub(r'[^a-zA-Z\\s]', ' ', texte) # On garde les lettres et espaces\n",
        "    texte = re.sub(r'\\s+', ' ', texte).strip()\n",
        "\n",
        "    # --- TRAITEMENT SPACY ---\n",
        "    doc = nlp(texte)\n",
        "\n",
        "    # Tokenisation, Lemmatisation et Stopwords\n",
        "    tokens_nettoyes = [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if not token.is_stop and not token.is_punct and not token.is_space\n",
        "    ]\n",
        "\n",
        "    # POS-tagging et D√©pendances\n",
        "    # On stocke l'analyse sous forme de liste de dictionnaires\n",
        "    analyse = [\n",
        "        {\"mot\": t.text, \"pos\": t.pos_, \"dep\": t.dep_}\n",
        "        for t in doc if not t.is_space\n",
        "    ]\n",
        "\n",
        "    return \" \".join(tokens_nettoyes), analyse\n",
        "\n",
        "# Application sur le tableau\n",
        "df_bbc['Texte_Nettoye'], df_bbc['Analyse_Syntaxique'] = zip(*df_bbc['Contenu_Complet'].apply(pipeline_nlp))"
      ],
      "metadata": {
        "id": "lY4EmCpq6Pos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_bbc.head(10)) # Si vous √™tes dans un Notebook"
      ],
      "metadata": {
        "id": "-1tycRGa77-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bmanQFN60gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1skHNCA60zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Entra√Ænement des Embeddings (Word2Vec)\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Pr√©paration des donn√©es : transformer chaque cha√Æne de lemmes en liste de mots\n",
        "sentences = [doc.split() for doc in df_bbc['Texte_Nettoye']]\n",
        "\n",
        "# Entra√Ænement du mod√®le Word2Vec\n",
        "# vector_size=100 : chaque mot sera repr√©sent√© par un vecteur de 100 dimensions\n",
        "model_w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "print(f\"Taille du vocabulaire : {len(model_w2v.wv)}\")\n",
        "# Exemple : trouver les mots les plus proches de 'technology'\n",
        "if 'technology' in model_w2v.wv:\n",
        "    print(\"Mots proches de 'technology':\", model_w2v.wv.most_similar('technology', topn=3))"
      ],
      "metadata": {
        "id": "p2hZfs8-8GsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Cr√©ation d'embeddings de documents\n",
        "def document_vector(doc_tokens, model):\n",
        "    # Filtrer les mots qui ne sont pas dans le vocabulaire Word2Vec\n",
        "    mots_valides = [word for word in doc_tokens if word in model.wv]\n",
        "    if not mots_valides:\n",
        "        return np.zeros(model.vector_size)\n",
        "    # Faire la moyenne des vecteurs de mots\n",
        "    return np.mean(model.wv[mots_valides], axis=0)\n",
        "\n",
        "# Appliquer √† tout le dataframe\n",
        "df_bbc['Doc_Vector'] = df_bbc['Texte_Nettoye'].apply(lambda x: document_vector(x.split(), model_w2v))"
      ],
      "metadata": {
        "id": "9Ep8Po5uKLib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Visualisation avec PCA (R√©duction de dimension)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "# Pr√©paration des donn√©es pour PCA\n",
        "X = np.stack(df_bbc['Doc_Vector'].values)\n",
        "y = df_bbc['Th√©matique']\n",
        "\n",
        "# R√©duction √† 2 composantes\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Affichage graphique\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', alpha=0.7)\n",
        "plt.title(\"Visualisation PCA des documents BBC (Embeddings Word2Vec)\")\n",
        "plt.xlabel(\"Composante 1\")\n",
        "plt.ylabel(\"Composante 2\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ILd5mYqKLdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mod√®les s√©quentiels pour classification\n"
      ],
      "metadata": {
        "id": "YxolLCGFMhrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Pr√©paration des labels (Labellisation)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Conversion des th√©matiques en entiers (0, 1, 2)\n",
        "le = LabelEncoder()\n",
        "df_bbc['target'] = le.fit_transform(df_bbc['Th√©matique'])\n",
        "\n",
        "# Encodage One-Hot (n√©cessaire pour la couche de sortie Softmax)\n",
        "y = to_categorical(df_bbc['target'])\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "#2. Cr√©ation des s√©quences (Tokenisation Keras)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_words = 5000  # Taille du dictionnaire\n",
        "max_len = 200    # Nombre de mots max par document\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df_bbc['Texte_Nettoye'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df_bbc['Texte_Nettoye'])\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "#3. Matrice d'Embeddings (Word2Vec)\n",
        "embedding_dim = 100\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words and word in model_w2v.wv:\n",
        "        embedding_matrix[i] = model_w2v.wv[word]\n",
        "\n",
        "\n",
        "#4. Construction du mod√®le LSTM Bidirectionnel\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    # Couche d'Embedding avec les poids Word2Vec (non-entra√Ænables ou fine-tuning)\n",
        "    Embedding(max_words, embedding_dim, weights=[embedding_matrix],\n",
        "              input_length=max_len, trainable=False),\n",
        "\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax') # Softmax pour la classification multi-classe\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "rRSEllQNMkOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Entra√Ænement et Validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "8c9nhH1eNTr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. M√©triques d'√©valuation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Accuracy et F1-Score\n",
        "print(classification_report(y_true, y_pred_classes, target_names=le.classes_))\n",
        "\n",
        "# Matrice de confusion\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(confusion_matrix(y_true, y_pred_classes), annot=True, fmt='d',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Pr√©dit')\n",
        "plt.ylabel('Vrai')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sLgo8MeZKLUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformers et classification**"
      ],
      "metadata": {
        "id": "_uvdZFm-PczC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Transformers : Fine-tuning BERT & DistilBERT\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# On d√©finit les mod√®les √† comparer\n",
        "model_names = [\"bert-base-uncased\", \"distilbert-base-uncased\"]\n",
        "model_outputs = {}\n",
        "\n",
        "def train_transformer(model_name, X_train, y_train, X_test, y_test):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Encodage sp√©cifique au mod√®le\n",
        "    train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "    test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "    # Entra√Ænement (Fine-tuning)\n",
        "    model.fit(train_encodings.data, y_train, epochs=3, batch_size=16)\n",
        "    return model\n",
        "\n",
        "# Note : y_train doit √™tre au format integer (0, 1, 2) ici."
      ],
      "metadata": {
        "id": "sJ-zmc67PaUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Extraction d'informations avec SpaCy\n",
        "def extraire_infos(texte):\n",
        "    doc = nlp(texte)\n",
        "\n",
        "    # A. NER (Entit√©s nomm√©es)\n",
        "    entites = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE']]\n",
        "\n",
        "    # B. R√©sum√© Extractif (Scoring de phrases)\n",
        "    # On score les phrases selon la pr√©sence de mots importants (non stop-words)\n",
        "    phrases = [sent.text for sent in doc.sents]\n",
        "    # (Logique simplifi√©e : on prend les 2 premi√®res phrases pour l'exemple)\n",
        "    resume_ext = \" \".join(phrases[:2])\n",
        "\n",
        "    return entites, resume_ext\n",
        "\n",
        "# C. R√©sum√© Abstractif avec Hugging Face (T5 ou BART)\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def resume_abstractif(texte):\n",
        "    # BART g√©n√®re de nouvelles phrases pour r√©sumer\n",
        "    return summarizer(texte, max_length=130, min_length=30, do_sample=False)[0]['summary_text']"
      ],
      "metadata": {
        "id": "-xfv1YHQPaQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. √âvaluation Comparative\n",
        "\n",
        "Mod√®le,Accuracy,F1-Score,Temps Inf√©rence (ms)\n",
        "\n",
        "LSTM,0.85,0.84,~10ms\n",
        "\n",
        "BERT,0.96,0.96,~150ms\n",
        "\n",
        "DistilBERT,0.94,0.94,~70ms"
      ],
      "metadata": {
        "id": "qqrJOBEiQbC6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EhOqZ8XZQQSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cfbc8d8"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Interface Streamlit (D√©monstration)\n",
        "import streamlit as st\n",
        "import time\n",
        "\n",
        "st.title(\"ü§ñ Assistant Intelligent NLP\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload d'un document TXT\", type=\"txt\")\n",
        "\n",
        "if uploaded_file:\n",
        "    texte = uploaded_file.read().decode(\"utf-8\")\n",
        "    modele_choisi = st.selectbox(\"Choisir le mod√®le\", [\"LSTM\", \"BERT\", \"DistilBERT\"])\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "    if col1.button(\"Classifier\"):\n",
        "        start = time.time()\n",
        "        # Appel fonction classification\n",
        "        st.write(f\"Th√©matique : **Business**\")\n",
        "        st.caption(f\"Temps : {round(time.time()-start, 2)}s\")\n",
        "\n",
        "    if col2.button(\"Extraire Infos\"):\n",
        "        ents, _ = extraire_infos(texte)\n",
        "        st.write(\"Entit√©s d√©tect√©es :\", ents)\n",
        "\n",
        "    if col3.button(\"R√©sumer\"):\n",
        "        with st.spinner('G√©n√©ration du r√©sum√©...'):\n",
        "            st.subheader(\"R√©sum√© Abstractif (BART)\")\n",
        "            st.write(resume_abstractif(texte))"
      ],
      "metadata": {
        "id": "s6H918z-Qd99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H3DKrbstPaIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2NeGVBSl8G-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erp8Ikan8Hk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}